{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "119615eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274aed77",
   "metadata": {},
   "source": [
    "Extract the zip file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2c8e8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCLRuleMonitor(tf.keras.callbacks.Callback):\n",
    "  def __init__(self, CL):\n",
    "    super(MyCLRuleMonitor).__init__()\n",
    "    self.CL = CL\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    trainScore = logs[\"accuracy\"]\n",
    "    testScore = logs[\"val_accuracy\"]\n",
    "\n",
    "    if testScore > trainScore and testScore >= self.CL:\n",
    "      self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dcad541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extacted the file\n"
     ]
    }
   ],
   "source": [
    "zip_name = 'fruits-small.zip'\n",
    "extracted_file = 'fruits-small'\n",
    "\n",
    "# Make new directory:\n",
    "os.makedirs(extracted_file, exist_ok = True)\n",
    "\n",
    "try:\n",
    "    shutil.unpack_archive(filename= zip_name, extract_dir= extracted_file)\n",
    "    print('Successfully extacted the file')\n",
    "except Exception as e:\n",
    "    print('Error while unpacking the zip file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a1cf49",
   "metadata": {},
   "source": [
    "Steps for Model creation:\n",
    "0. Data Preprocessing\n",
    "    i. Image Generators\n",
    "    ii. Splitting Train and Test data from Generators\n",
    "1. Model architecting + Prepro\n",
    "    i. CNN + ANN layers creation\n",
    "2. Model Compilation\n",
    "3. Model Training \n",
    "4. Model Evaluation\n",
    "5. Model Testing / Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7915df8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data compatible using ImageDataGenerators:\n",
    "# Each image in the Folders will be normalized using rescale.\n",
    "train_image_generator= tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1.0/255.0)\n",
    "test_image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale= 1.0/255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a57130c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3425 images belonging to 7 classes.\n",
      "Found 1150 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Splitting Test and Train Data:\n",
    "\n",
    "train_image_data = train_image_generator.flow_from_directory('fruits-small/fruits-small/data/Training',\n",
    "                                                             batch_size= 3,\n",
    "                                                             class_mode = 'categorical',\n",
    "                                                             target_size = (224,224))\n",
    "\n",
    "\n",
    "test_image_data = test_image_generator.flow_from_directory('fruits-small/fruits-small/data/Validation',\n",
    "                                                           batch_size= 3,\n",
    "                                                           class_mode = 'categorical',\n",
    "                                                           target_size= (224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3733e732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image_data.image_shape\n",
    "test_image_data.image_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cd0c725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thedo\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100352</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">25,690,368</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,799</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │         \u001b[38;5;34m9,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100352\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │    \u001b[38;5;34m25,690,368\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m65,664\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │         \u001b[38;5;34m1,799\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,932,583</span> (98.92 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,932,583\u001b[0m (98.92 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,932,583</span> (98.92 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,932,583\u001b[0m (98.92 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model Architecting:\n",
    "# CNN: A Convolution layer is combination of Convolve and Pooling:\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Conv2d(noofFeatureMaps, kernelShape, inputShape, activation) + padding = same -- add virtual boarder in every image\n",
    "# NoOfFeatureMaps: Similar to no of units: Based on trial and error\n",
    "# filter: (3,3), (4,4), (5,5) common :  in this content will be initialized randomly\n",
    "# inputShape: similar to target_size\n",
    "\n",
    "# ----------------- 1st convole layer:\n",
    "model.add(tf.keras.layers.Conv2D(32, (3,3), input_shape= train_image_data.image_shape, activation='relu', padding= 'same'))\n",
    "# Adding Pooling:\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "# ------------------- 2nd Convole layer:\n",
    "model.add(tf.keras.layers.Conv2D(32, (3,3), activation='relu', padding= 'same'))\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size = (2,2)))\n",
    "\n",
    "# -------------------- Flatten:\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# FC Layer | ANN part, Layers creation:\n",
    "\n",
    "model.add(tf.keras.layers.Dense(units= 256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(units= 512, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(units= 128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(units= 256, activation= 'relu'))\n",
    "\n",
    "# Output layer:\n",
    "model.add(tf.keras.layers.Dense(units = 7, activation='softmax'))\n",
    "\n",
    "# Model Summay:\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab877155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COmpilation:\n",
    "# Balanced dat is there\n",
    "# A balanced dataset, in the context of machine learning classification tasks, is a dataset in which every class or category is represented by an approximately equal number of samples. \n",
    "model.compile(optimizer= 'adam',\n",
    "              loss= 'categorical_crossentropy',\n",
    "              metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f82b41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1141/1141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m585s\u001b[0m 510ms/step - accuracy: 0.9117 - loss: 0.3216 - val_accuracy: 0.6597 - val_loss: 0.9680\n",
      "Epoch 2/100\n",
      "\u001b[1m   1/1141\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:09\u001b[0m 430ms/step - accuracy: 0.3333 - loss: 1.7754"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thedo\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1141/1141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.3333 - loss: 1.7754 - val_accuracy: 0.6641 - val_loss: 0.9460\n",
      "Epoch 3/100\n",
      "\u001b[1m1141/1141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m514s\u001b[0m 450ms/step - accuracy: 0.9623 - loss: 0.1530 - val_accuracy: 0.8721 - val_loss: 0.9520\n",
      "Epoch 4/100\n",
      "\u001b[1m1141/1141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.4914e-05 - val_accuracy: 0.8190 - val_loss: 2.1108\n",
      "Epoch 5/100\n",
      "\u001b[1m1141/1141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m476s\u001b[0m 417ms/step - accuracy: 0.9921 - loss: 0.0333 - val_accuracy: 0.9843 - val_loss: 0.0520\n",
      "Epoch 6/100\n",
      "\u001b[1m1141/1141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 0.9843 - val_loss: 0.0520\n",
      "Epoch 7/100\n",
      "\u001b[1m1141/1141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m523s\u001b[0m 458ms/step - accuracy: 1.0000 - loss: 5.8128e-06 - val_accuracy: 0.9878 - val_loss: 0.0376\n",
      "Epoch 8/100\n",
      "\u001b[1m1141/1141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 0.9878 - val_loss: 0.0376\n",
      "Epoch 9/100\n",
      "\u001b[1m1141/1141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40067s\u001b[0m 35s/step - accuracy: 1.0000 - loss: 9.4691e-07 - val_accuracy: 0.9930 - val_loss: 0.0248\n",
      "Epoch 10/100\n",
      "\u001b[1m1141/1141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 0.9930 - val_loss: 0.0249\n",
      "Epoch 11/100\n",
      "\u001b[1m1141/1141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m487s\u001b[0m 426ms/step - accuracy: 1.0000 - loss: 1.3509e-07 - val_accuracy: 0.9948 - val_loss: 0.0227\n",
      "Epoch 12/100\n",
      "\u001b[1m1141/1141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 1.1921e-07 - val_accuracy: 0.9948 - val_loss: 0.0227\n",
      "Epoch 13/100\n",
      "\u001b[1m1141/1141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m524s\u001b[0m 459ms/step - accuracy: 1.0000 - loss: 4.9293e-08 - val_accuracy: 0.9948 - val_loss: 0.0234\n",
      "Epoch 14/100\n",
      "\u001b[1m1141/1141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 0.9948 - val_loss: 0.0234\n",
      "Epoch 15/100\n",
      "\u001b[1m1141/1141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32708s\u001b[0m 29s/step - accuracy: 1.0000 - loss: 2.2156e-08 - val_accuracy: 0.9948 - val_loss: 0.0227\n",
      "Epoch 16/100\n",
      "\u001b[1m1141/1141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 7.9473e-08 - val_accuracy: 0.9948 - val_loss: 0.0227\n",
      "Epoch 17/100\n",
      "\u001b[1m1141/1141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m535s\u001b[0m 468ms/step - accuracy: 1.0000 - loss: 1.1670e-08 - val_accuracy: 0.9948 - val_loss: 0.0227\n",
      "Epoch 18/100\n",
      "\u001b[1m1141/1141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 3.9736e-08 - val_accuracy: 0.9948 - val_loss: 0.0227\n",
      "Epoch 19/100\n",
      "\u001b[1m1141/1141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 428ms/step - accuracy: 1.0000 - loss: 6.9672e-09 - val_accuracy: 0.9948 - val_loss: 0.0222\n",
      "Epoch 20/100\n",
      "\u001b[1m1141/1141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 0.9948 - val_loss: 0.0222\n",
      "Epoch 21/100\n",
      "\u001b[1m1141/1141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m508s\u001b[0m 445ms/step - accuracy: 1.0000 - loss: 4.0758e-09 - val_accuracy: 0.9948 - val_loss: 0.0221\n",
      "Epoch 22/100\n",
      "\u001b[1m1141/1141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 0.9948 - val_loss: 0.0221\n",
      "Epoch 23/100\n",
      "\u001b[1m 569/1141\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3:53\u001b[0m 409ms/step - accuracy: 1.0000 - loss: 3.6543e-09"
     ]
    }
   ],
   "source": [
    "# Model training:\n",
    "\n",
    "model.fit(train_image_data, \n",
    "          validation_data = test_image_data, \n",
    "          epochs = 100, \n",
    "          steps_per_epoch = (len(train_image_data.filenames)//train_image_data.batch_size ),\n",
    "          validation_steps= (len(test_image_data.filenames)//test_image_data.batch_size),\n",
    "          callbacks = MyCLRuleMonitor(0.9))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668b3aa5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Input / Deployment / Testing:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m image = \u001b[43mtf\u001b[49m.keras.preprocessing.image.load_img(\u001b[33m'\u001b[39m\u001b[33mfazli-mango.jpg\u001b[39m\u001b[33m'\u001b[39m, target_size = (\u001b[32m224\u001b[39m,\u001b[32m224\u001b[39m))\n\u001b[32m      4\u001b[39m image_array = tf.keras.preprocessing.image.img_to_array(image)\n\u001b[32m      6\u001b[39m image_np_array = np.expand_dims(image_array, axis = \u001b[32m0\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Input / Deployment / Testing:\n",
    "\n",
    "image = tf.keras.preprocessing.image.load_img('fazli-mango.jpg', target_size = (224,224))\n",
    "image_array = tf.keras.preprocessing.image.img_to_array(image)\n",
    "\n",
    "image_np_array = np.expand_dims(image_array, axis = 0)\n",
    "prediction_probabilities = model.predict(image_np_array)\n",
    "\n",
    "train_dir = 'fruits-small/fruits-small/data/Training'\n",
    "class_names = sorted(os.listdir(train_dir))\n",
    "\n",
    "prediction_class_index= np.argmax(prediction_probabilities)\n",
    "prediction_class_name = class_names[prediction_class_index]\n",
    "print(prediction_class_index)\n",
    "print(class_names)\n",
    "print(prediction_class_name)\n",
    "\n",
    "probability_prediction = model.predict(image_np_array)\n",
    "# For binary classification with sigmoid output, we apply a threshold\n",
    "# If probability_prediction[0][0] >= 0.5, it's class 1 (dogs), otherwise class 0 (cats)\n",
    "\n",
    "if probability_prediction[0][0] >= 0.5:\n",
    "    prediction_class_index = 1  # Corresponds to 'dogs'\n",
    "else:\n",
    "    prediction_class_index = 0  # Corresponds to 'cats'\n",
    "\n",
    "print(f\"Raw probability: {probability_prediction[0][0]:.4f}\")\n",
    "\n",
    "class_names_map = {v: k for k, v in train_image_data.class_indices.items()}\n",
    "\n",
    "# Use the index to look up the actual class name\n",
    "predicted_class_name = class_names_map[prediction_class_index]\n",
    "\n",
    "# Print the result\n",
    "print(f\"The predicted pet name is: {predicted_class_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd598fc1",
   "metadata": {},
   "source": [
    "It is not able to predict a Mango and giving res as Banana, So WIll be playing with the batch size and let it run again, then we will try again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c389f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3ec126c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
