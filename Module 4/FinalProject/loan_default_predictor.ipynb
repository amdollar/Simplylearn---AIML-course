{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad8fad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1229e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load data\n",
    "def load_data(data_path):\n",
    "    try:\n",
    "        data = pd.read_csv(data_path)\n",
    "        print(f'Dataset loaded: {data.shape[0]} rows, {data.shape[1]} columns.')\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f'File was not found: {data_path}')\n",
    "        return None\n",
    "    \n",
    "def validate_data(df):\n",
    "    'Validate data structure and quality'\n",
    "\n",
    "    validation_report = {\n",
    "        'total_rows': len(df),\n",
    "        'total_cols': len(df.columns),\n",
    "        'missing_values': df.isnull().sum().to_dict(),\n",
    "        'has_target': 'not.fully.paid' in df.columns\n",
    "    }\n",
    "\n",
    "    print('Data validation completed')\n",
    "    return validation_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33e5c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Display data information:\n",
    "def display_data_info(data):\n",
    "    '''Display basic information about the dataset: '''\n",
    "\n",
    "    print('\\n' + '=' * 60)\n",
    "    print('Dataset Information')\n",
    "    print('=' * 60)\n",
    "\n",
    "    print(f'Shape: {data.shape}')\n",
    "    print('\\n Column names and data type:')\n",
    "    \n",
    "    for col, dtype in data.dtypes.items():\n",
    "        print(f' {col}: {dtype}')\n",
    "    \n",
    "    print(f'\\n Missing values: ')\n",
    "    missing = data.isnull().sum()\n",
    "    if missing.sum() == 0:\n",
    "        print('No missing values')\n",
    "    else:\n",
    "        print(missing[missing > 0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5f631c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 2: \n",
    "def analyze_numerical_features(df):\n",
    "    \"Analyse statistical properties of numerical features\"\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "    print('\\n' + '=' * 60)\n",
    "    print('Numerical features analysis')\n",
    "    print('=' * 60)\n",
    "    stats = df[numerical_cols].describe()\n",
    "    print(stats)\n",
    "    return stats\n",
    "\n",
    "def analyze_categorical_features(df):\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    print('\\n' + '=' * 60)\n",
    "    print('Categorical features analysis')\n",
    "    print('=' * 60)\n",
    "    for col in categorical_cols:\n",
    "        print(f'\\n {col} (Value Counts):')\n",
    "        print(df[col].value_counts())\n",
    "    return categorical_cols.tolist()\n",
    "\n",
    "def analyze_target_distribution(df):\n",
    "    '''Analyze the target variable distribution'''\n",
    "    target_col = 'not.fully.paid'\n",
    "\n",
    "    if target_col in df.columns:\n",
    "        print('\\n' + '=' * 60)\n",
    "        print('Target variable distribution')\n",
    "        print('=' * 60)\n",
    "\n",
    "        distribution = df[target_col].value_counts()\n",
    "        percentage = (df[target_col].value_counts(normalize = True) * 100)\n",
    "        print(f'Fully Paid (0): {distribution[0]} ({percentage[0]:.2f}%)')\n",
    "        print(f'Defaulted (0): {distribution[1]} ({percentage[1]:.2f}%)')\n",
    "\n",
    "        return distribution\n",
    "\n",
    "\n",
    "def generate_eda_report(df):\n",
    "    '''Generate complete EDA report:'''\n",
    "    print('\\n' + '=' * 60)\n",
    "    print('Exploratory data Analysis report')\n",
    "    print('=' * 60)\n",
    "\n",
    "    analyze_numerical_features(df)\n",
    "    analyze_categorical_features(df)\n",
    "    analyze_target_distribution(df)\n",
    "\n",
    "    print('Eda Report Generated')\n",
    "\n",
    "def create_visualization(df, output_dir):\n",
    "    '''Create and save visualization'''\n",
    "\n",
    "    #1. Target Distribution\n",
    "    plt.figure(figsize = (10,5))\n",
    "    df['not.fully.paid'].value_counts().plot(kind= 'bar')\n",
    "    plt.title('Loan default distribution', fontsize = 14, fontweight= 'bold')\n",
    "    plt.xlabel('Not full paid (0=Paid, 1=Defaulted)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/target_distribution.png', dpi=100)\n",
    "    plt.close()\n",
    "\n",
    "    # Interest rate distribution\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.hist(df['int.rate'], bins=50, edgecolor='black')\n",
    "    plt.title('Interest rate distribution', fontsize= 14, fontweight= 'bold')\n",
    "    plt.xlabel('Interest Rate')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/interest_rate_distribution.png', dpi= 100)\n",
    "    plt.close()\n",
    "\n",
    "    # 3. FICO score distribution\n",
    "    plt.figure(figsize = (10,5))\n",
    "    plt.hist(df['fico'], bins = 50, edgecolor='black')\n",
    "    plt.title('FICO score distribution', fontsize=14, fontweight = 'bold')\n",
    "    plt.xlabel('FICO score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/fico_distribution.png', dpi=100)\n",
    "    plt.close()\n",
    "\n",
    "    print(f'Visualizations saved to {output_dir}/')\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02d65d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 3: Feature Correlation analysis\n",
    "def calculate_correlation_matrix(df):\n",
    "    '''Calculate Pearson correlation matrix'''\n",
    "\n",
    "    numerical_cols = df.select_dtypes(include=[np.numbers]).columns\n",
    "    corr_matrix = df[numerical_cols].corr()\n",
    "    print('\\n Correlation matrix calculated')\n",
    "    return corr_matrix\n",
    "\n",
    "def identify_high_correlation_features(df, threshold = 0.9):\n",
    "    '''Identify highly correlated feature pairs'''\n",
    "    corr_matrix = df.corr(numeric_only = True)\n",
    "    high_corr_pairs = []\n",
    "\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1 , len(corr_matrix.columns)):\n",
    "            if abs(corr_matrix.iloc[i,j]) > threshold:\n",
    "                high_corr_pairs.append({\n",
    "                    'feature1': corr_matrix.columns[i],\n",
    "                    'feature2': corr_matrix.columns[j],\n",
    "                    'correlation': corr_matrix.iloc[i,j]\n",
    "                })\n",
    "\n",
    "    if high_corr_pairs: \n",
    "        print(f'Found {len(high_corr_pairs)} highly correlation pairs (threshold= {threshold}):')\n",
    "        for pair in high_corr_pairs:\n",
    "            print(f'{pair['feature1']} <-> {pair['feature2']}: {pair['correlation']:.3f}')\n",
    "    else: \n",
    "        print(f' No Highly correlated featues found (threshold = {threshold})')\n",
    "\n",
    "    return high_corr_pairs\n",
    "\n",
    "def analyze_feature_importance_with_target(df):\n",
    "    '''Analyze correlation of features with target variable'''\n",
    "    target_col = 'not.fully.paid'\n",
    "    feature_cols = [col for col in df.columns if col != target_col]\n",
    "\n",
    "    correlations = []\n",
    "\n",
    "    for col in feature_cols:\n",
    "        if df[col].dtypes in [np.int64, np.float64]:\n",
    "            corr = df[col].corr(df[target_col])\n",
    "            correlations.append({'feature': col, 'correlation': abs(corr)})\n",
    "    \n",
    "    # sort by abs correlation\n",
    "    correlations = sorted(correlations, key = lambda x: x['correlation'], reverse= True)\n",
    "\n",
    "    print('Feature importance (correlation with target): ')\n",
    "    for item in correlations[:10]:\n",
    "        print(f'{item['feature']}: {item['correlation']:.4f}')\n",
    "\n",
    "    return correlations\n",
    "\n",
    "def visualize_correlation(df, output_dir = 'outputs'):\n",
    "    '''Create and Save correlation heatmap'''\n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "\n",
    "    # Select numerical cols: \n",
    "    numerical_cols = df.select_dtypes(include= [np.number])\n",
    "    corr_matrix = numerical_cols.corr()\n",
    "\n",
    "    # Create Heatmap\n",
    "    plt.figure(figsize = (12, 10))\n",
    "    sns.heatmap(corr_matrix, annot= True, fmt = '.2f', cmap = 'coolwarm', center = 0)\n",
    "    plt.title('Feature correlation matrix', fontsize = 14, fontweight = 'bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/correlation_heatmap.png', dpi= 100)\n",
    "    plt.close()\n",
    "\n",
    "    print(f'Correlation heatmap saved to {output_dir}/correlation_heatmap.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9165ef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 4: Feature Reduction:\n",
    "def select_best_features(df, threshold = 0.9):\n",
    "    '''Select best features by removing highly correlated ones'''\n",
    "    target_col = 'not.fully.paid'\n",
    "    features_to_keep = [col for col in df.columns if col != target_col]\n",
    "\n",
    "    # Get correlation matrix\n",
    "    corr_matrix = df[features_to_keep].corr(numeric_only = True)\n",
    "    \n",
    "    # Remove highly correlated features:\n",
    "    to_drop = set()\n",
    "\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if abs(corr_matrix.iloc[i,j] > threshold):\n",
    "                to_drop.add(corr_matrix.columns[j])\n",
    "\n",
    "    selected_features = [col for col in features_to_keep if col not in to_drop]\n",
    "\n",
    "    print(f'\\nFeature Selection Complete:')\n",
    "    print(f'Orignal Features: {len(features_to_keep)}')\n",
    "    print(f'Removed Features: {len(to_drop)} {list(to_drop)}')\n",
    "    print(f'Selected Features: {len(selected_features)} {selected_features}')\n",
    "\n",
    "    return selected_features\n",
    "\n",
    "def reduce_features(df, correlation_threshold = 0.9):\n",
    "    '''Complete feature reduction pipeline'''\n",
    "\n",
    "    print('\\n' + '=' * 60)\n",
    "    print('Feature Reduction')\n",
    "    print('=' * 60)\n",
    "\n",
    "    # Identify correlations\n",
    "    identify_high_correlation_features(df, threshold=correlation_threshold)\n",
    "\n",
    "    # Analyze importance\n",
    "    analyze_feature_importance_with_target(df)\n",
    "\n",
    "    # Select Best Features\n",
    "    best_features = select_best_features(df, threshold=correlation_threshold)\n",
    "\n",
    "    return best_features\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1ff1919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical_features(df):\n",
    "    df_encoded = df.copy()\n",
    "    # Find categorical columns (Excluding target)\n",
    "    categorical_cols = df_encoded.select_dtypes(include= ['object']).columns.tolist()\n",
    "    print(f'Encoding categorical columns: {categorical_cols}')\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        df_encoded = pd.get_dummies(df_encoded, columns=[col], prefix= col, drop_first=True)\n",
    "\n",
    "    print(f'Categorical encoding completed.')\n",
    "    return df_encoded\n",
    "\n",
    "\n",
    "def handle_missing_values(df, strategy= 'drop'):\n",
    "    if df.isnull().sum().sum() == 0:\n",
    "        print('No Null values to handle.')\n",
    "        return df\n",
    "    \n",
    "    if strategy == 'drop':\n",
    "        df_clean = df.dropna()\n",
    "        print(f'Dropped {len(df) - len(df_clean)} rows with missing values.')\n",
    "    else:\n",
    "        numerical_cols = df.select_dtypes(include= [np.number]).columns\n",
    "        df_clean = df.copy()\n",
    "        for col in numerical_cols:\n",
    "            df_clean[col].fillna(df_clean[col].mean(), inplace = True)\n",
    "        print('Filled null column values with mean.')\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def scale_features(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    print('Features scaled using Standard Scaler')\n",
    "    return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "\n",
    "def prepare_dataset(df, test_size = 0.2, random_state= 42):\n",
    "    '''Complete data prepration pipeline'''\n",
    "    print('\\n' + '=' * 60)\n",
    "    print('Data Preprocessing')\n",
    "    print('=' * 60)\n",
    "\n",
    "    # Handle missing values : Numerical columns\n",
    "    df = handle_missing_values(df)\n",
    "\n",
    "    # Eocode categorical features:\n",
    "    df = encode_categorical_features(df)\n",
    "\n",
    "    # Saperate feature and target:\n",
    "    target_col = 'not.fully.paid'\n",
    "    y = df[target_col].values\n",
    "    X = df.drop(columns=[target_col]).values\n",
    "\n",
    "    # Split Features:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= test_size, random_state= random_state)\n",
    "\n",
    "    print(f'Data Split:')\n",
    "    print(f'Training Set: {X_train.shape}')\n",
    "    print(f'Testing Set: {X_test.shape}')\n",
    "\n",
    "    # Scale features:\n",
    "    X_train_scaled, X_test_scaled, scaler = scale_features(X_train, X_test)\n",
    "\n",
    "    return {\n",
    "        'X_train': X_train_scaled,\n",
    "        'X_test': X_test_scaled,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'scaler': scaler,\n",
    "        'input_shape': X_train_scaled.shape[1]\n",
    "    }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e3242af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 6: Deep Learning Model\n",
    "\n",
    "def build_neural_network(input_dim):\n",
    "    '''Build Neural Network Model'''\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation= 'relu', input_dim= input_dim),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer = keras.optimizers.Adam(learning_rate= 0.001), loss= 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    print('Neural network model built')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ff63fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(model, X_train, y_train, X_test, y_test, epochs = 50, batch_size = 32):\n",
    "    ''''Train Neural network'''\n",
    "    # Early stopping callback:\n",
    "\n",
    "    early_stop = callbacks.EarlyStopping(\n",
    "        monitor= 'val_loss',\n",
    "        patience = 5, \n",
    "        restore_best_weights = True\n",
    "    )\n",
    "\n",
    "    # Learning rate reduction:\n",
    "    lr_reduce = callbacks.ReduceLROnPlateau(\n",
    "        monitor = 'val_loss',\n",
    "        factor = 0.5,\n",
    "        patience = 3,\n",
    "        min_lr = 1e-7\n",
    "    )\n",
    "\n",
    "    print('\\n' + '=' * 60)\n",
    "    print('Model Training')\n",
    "    print('=' * 60)\n",
    "\n",
    "    # train model:\n",
    "    history = model.fit(X_train, y_train, epochs = epochs, batch_size = batch_size, \n",
    "                        validation_data =  (X_test, y_test), callbacks= [early_stop, lr_reduce],\n",
    "                        verbose = 1)\n",
    "    \n",
    "    print('Training completed')\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69615325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Lending club loan default Prediction\n",
      "============================================================\n",
      "\n",
      " Step 1: Loading Data...\n",
      "Dataset loaded: 9578 rows, 14 columns.\n",
      "\n",
      "============================================================\n",
      "Dataset Information\n",
      "============================================================\n",
      "Shape: (9578, 14)\n",
      "\n",
      " Column names and data type:\n",
      " credit.policy: int64\n",
      " purpose: object\n",
      " int.rate: float64\n",
      " installment: float64\n",
      " log.annual.inc: float64\n",
      " dti: float64\n",
      " fico: int64\n",
      " days.with.cr.line: float64\n",
      " revol.bal: int64\n",
      " revol.util: float64\n",
      " inq.last.6mths: int64\n",
      " delinq.2yrs: int64\n",
      " pub.rec: int64\n",
      " not.fully.paid: int64\n",
      "\n",
      " Missing values: \n",
      "No missing values\n",
      "Data validation completed\n",
      "\n",
      " Step 2: Perform EDA: \n",
      "\n",
      "============================================================\n",
      "Exploratory data Analysis report\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Numerical features analysis\n",
      "============================================================\n",
      "       credit.policy     int.rate  installment  log.annual.inc          dti  \\\n",
      "count    9578.000000  9578.000000  9578.000000     9578.000000  9578.000000   \n",
      "mean        0.804970     0.122640   319.089413       10.932117    12.606679   \n",
      "std         0.396245     0.026847   207.071301        0.614813     6.883970   \n",
      "min         0.000000     0.060000    15.670000        7.547502     0.000000   \n",
      "25%         1.000000     0.103900   163.770000       10.558414     7.212500   \n",
      "50%         1.000000     0.122100   268.950000       10.928884    12.665000   \n",
      "75%         1.000000     0.140700   432.762500       11.291293    17.950000   \n",
      "max         1.000000     0.216400   940.140000       14.528354    29.960000   \n",
      "\n",
      "              fico  days.with.cr.line     revol.bal   revol.util  \\\n",
      "count  9578.000000        9578.000000  9.578000e+03  9578.000000   \n",
      "mean    710.846314        4560.767197  1.691396e+04    46.799236   \n",
      "std      37.970537        2496.930377  3.375619e+04    29.014417   \n",
      "min     612.000000         178.958333  0.000000e+00     0.000000   \n",
      "25%     682.000000        2820.000000  3.187000e+03    22.600000   \n",
      "50%     707.000000        4139.958333  8.596000e+03    46.300000   \n",
      "75%     737.000000        5730.000000  1.824950e+04    70.900000   \n",
      "max     827.000000       17639.958330  1.207359e+06   119.000000   \n",
      "\n",
      "       inq.last.6mths  delinq.2yrs      pub.rec  not.fully.paid  \n",
      "count     9578.000000  9578.000000  9578.000000     9578.000000  \n",
      "mean         1.577469     0.163708     0.062122        0.160054  \n",
      "std          2.200245     0.546215     0.262126        0.366676  \n",
      "min          0.000000     0.000000     0.000000        0.000000  \n",
      "25%          0.000000     0.000000     0.000000        0.000000  \n",
      "50%          1.000000     0.000000     0.000000        0.000000  \n",
      "75%          2.000000     0.000000     0.000000        0.000000  \n",
      "max         33.000000    13.000000     5.000000        1.000000  \n",
      "\n",
      "============================================================\n",
      "Categorical features analysis\n",
      "============================================================\n",
      "\n",
      " purpose (Value Counts):\n",
      "purpose\n",
      "debt_consolidation    3957\n",
      "all_other             2331\n",
      "credit_card           1262\n",
      "home_improvement       629\n",
      "small_business         619\n",
      "major_purchase         437\n",
      "educational            343\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "Target variable distribution\n",
      "============================================================\n",
      "Fully Paid (0): 8045 (83.99%)\n",
      "Defaulted (0): 1533 (16.01%)\n",
      "Eda Report Generated\n",
      "\n",
      " Step 3: Analyzing feature analysis: \n",
      "Correlation heatmap saved to outputs/correlation_heatmap.png\n",
      "\n",
      " Step 4: Feature Reduction: \n",
      "\n",
      "============================================================\n",
      "Feature Reduction\n",
      "============================================================\n",
      " No Highly correlated featues found (threshold = 0.9)\n",
      "Feature importance (correlation with target): \n",
      "int.rate: 0.1596\n",
      "credit.policy: 0.1581\n",
      "fico: 0.1497\n",
      "inq.last.6mths: 0.1495\n",
      "revol.util: 0.0821\n",
      "revol.bal: 0.0537\n",
      "installment: 0.0500\n",
      "pub.rec: 0.0486\n",
      "dti: 0.0374\n",
      "log.annual.inc: 0.0334\n",
      "\n",
      "Feature Selection Complete:\n",
      "Orignal Features: 13\n",
      "Removed Features: 0 []\n",
      "Selected Features: 13 ['credit.policy', 'purpose', 'int.rate', 'installment', 'log.annual.inc', 'dti', 'fico', 'days.with.cr.line', 'revol.bal', 'revol.util', 'inq.last.6mths', 'delinq.2yrs', 'pub.rec']\n",
      "Dataset reduced from 14 to 14 columns.\n",
      "\n",
      " Step 5: Preprocessing data\n",
      "\n",
      "============================================================\n",
      "Data Preprocessing\n",
      "============================================================\n",
      "No Null values to handle.\n",
      "Encoding categorical columns: ['purpose']\n",
      "Categorical encoding completed.\n",
      "Data Split:\n",
      "Training Set: (7662, 18)\n",
      "Testing Set: (1916, 18)\n",
      "Features scaled using Standard Scaler\n",
      "\n",
      " Step 6: Building Neural Network...\n",
      "Neural network model built\n",
      "Model Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,432</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m2,432\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,801</span> (50.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m12,801\u001b[0m (50.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,801</span> (50.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,801\u001b[0m (50.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Step 7: Training Model...\n",
      "\n",
      "============================================================\n",
      "Model Training\n",
      "============================================================\n",
      "Epoch 1/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8365 - loss: 0.4494 - val_accuracy: 0.8408 - val_loss: 0.4059 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8387 - loss: 0.4287 - val_accuracy: 0.8413 - val_loss: 0.4091 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8403 - loss: 0.4245 - val_accuracy: 0.8408 - val_loss: 0.4122 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8406 - loss: 0.4214 - val_accuracy: 0.8413 - val_loss: 0.4128 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8397 - loss: 0.4187 - val_accuracy: 0.8408 - val_loss: 0.4044 - learning_rate: 5.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8400 - loss: 0.4144 - val_accuracy: 0.8408 - val_loss: 0.3996 - learning_rate: 5.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8393 - loss: 0.4166 - val_accuracy: 0.8408 - val_loss: 0.4009 - learning_rate: 5.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8404 - loss: 0.4143 - val_accuracy: 0.8408 - val_loss: 0.4038 - learning_rate: 5.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8406 - loss: 0.4113 - val_accuracy: 0.8408 - val_loss: 0.4011 - learning_rate: 5.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8410 - loss: 0.4102 - val_accuracy: 0.8403 - val_loss: 0.4015 - learning_rate: 2.5000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8408 - loss: 0.4084 - val_accuracy: 0.8403 - val_loss: 0.4013 - learning_rate: 2.5000e-04\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "def main (data_path = 'loan_data.csv', output_dir = 'outputs'):\n",
    "\n",
    "    'Execute complete pipeline:'\n",
    "\n",
    "    print('\\n' + '=' * 60)\n",
    "    print('Lending club loan default Prediction')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    # Step 1: Load and validate the data:\n",
    "    print('\\n Step 1: Loading Data...')\n",
    "\n",
    "    df = load_data(data_path)\n",
    "    if df is None:\n",
    "        print('Data set is empty')\n",
    "        return\n",
    "    \n",
    "    # Display data information\n",
    "\n",
    "    display_data_info(df)\n",
    "    validation = validate_data(df)\n",
    "\n",
    "    # Step 2: Performing EDA Exploratory Data Analysis\n",
    "    print('\\n Step 2: Perform EDA: ')\n",
    "    generate_eda_report(df)\n",
    "    # create_visualization(df, output_dir)\n",
    "\n",
    "    # Step 3: Feature Correlation Analysis\n",
    "    print('\\n Step 3: Analyzing feature analysis: ')\n",
    "    visualize_correlation(df, output_dir)\n",
    "\n",
    "    # Step 4: Reducing Features:\n",
    "    print('\\n Step 4: Feature Reduction: ')\n",
    "    best_features = reduce_features(df, correlation_threshold = 0.9)\n",
    "\n",
    "    # Keep target and selected features only\n",
    "    df_reduced = df[best_features + ['not.fully.paid']].copy()\n",
    "    print(f'Dataset reduced from {df.shape[1]} to {df_reduced.shape[1]} columns.')\n",
    "\n",
    "\n",
    "    # Step 5: Data preprocessing\n",
    "    print('\\n Step 5: Preprocessing data')\n",
    "    data_prep = prepare_dataset(df_reduced)\n",
    "\n",
    "    # Step 6: Build Model:\n",
    "    print('\\n Step 6: Building Neural Network...')\n",
    "    model = build_neural_network(data_prep['input_shape'])\n",
    "    print('Model Summary:')\n",
    "    model.summary()\n",
    "\n",
    "    # Step 7: Train model:\n",
    "    print('\\n Step 7: Training Model...')\n",
    "    history = train_neural_network(model, data_prep['X_train'], \n",
    "                                   data_prep['y_train'],\n",
    "                                   data_prep['X_test'],\n",
    "                                   data_prep['y_test'],\n",
    "                                   epochs = 50,\n",
    "                                   batch_size = 32)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    history = train_neural_network(\n",
    "        model, \n",
    "        data_pred['X_train'],\n",
    "        data_prep['y_train'],\n",
    "        data_pred['X_test'],\n",
    "        data_prep['y_test'],\n",
    "        epochs = 50,\n",
    "        batch_size = 32\n",
    "    )\n",
    "\n",
    "    # Plot training history\n",
    "    # plot_training_history(history, output_dir)\n",
    "    '''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
