{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HRAnalyzer:\n",
    "\n",
    "    def __init__(self, data_path):\n",
    "        \"Init with data path\"\n",
    "        self.data_path = data_path\n",
    "        self.df = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.models = {}\n",
    "        self.model_scores = {}\n",
    "\n",
    "    def load_data(self):\n",
    "        print('='*60)\n",
    "        print('Loding dataset')\n",
    "        print('='*60)\n",
    "\n",
    "        self.df = pd.read_csv(self.data_path)\n",
    "        print(f'Dataset shape: {self.df.shape}')\n",
    "        print(f'\\n First first 5 rows: ')\n",
    "        print(self.df.head(5))\n",
    "        print('\\n Dataset information: ')\n",
    "        print(self.df.info())\n",
    "\n",
    "\n",
    "    def data_quality_check(self):\n",
    "        '''Perform complete data quality check'''\n",
    "        print(\"=\" * 60)\n",
    "        print('Data quality checks')\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Check for missing values\n",
    "        missing_values = self.df.isnull().sum()\n",
    "        print(f'Number of missing values per column: {missing_values}')\n",
    "\n",
    "        if missing_values.sum() == 0:\n",
    "            print('No missing values found in the dataset!')\n",
    "        else:\n",
    "            print('Missing values found in the dataset!')\n",
    "\n",
    "        # Basic statistics analysis\n",
    "        print(f'\\n Basic Statistics: ')\n",
    "        print(self.df.describe())\n",
    "\n",
    "        # Check for duplicates:\n",
    "        print(f'Check for duplicates: ')\n",
    "        print(self.df.duplicated().sum())\n",
    "\n",
    "        return missing_values\n",
    "    \n",
    "    def exploratory_data_analytics(self):\n",
    "        'Peform comprehensive EDA as per requirement'\n",
    "        print('=' * 60)\n",
    "        print('2. Exploratory Data Analysis')\n",
    "        print('=' * 60)\n",
    "\n",
    "        # Corelation Heatmap\n",
    "        print(f' 2.1. Creation of corelation heatmap:')\n",
    "\n",
    "        plt.figure(figsize=(12,8))\n",
    "        numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "        correlation_matrix = self.df[numeric_cols].corr()\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap = 'coolwarm', center=0, square=True, fmt= '.2f')\n",
    "        plt.title('Correlation Matrix - Numerical Features', fontsize= 14, fontweight = 'bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('Correlationheatmap.png', dpi = 300, bbox_inches= 'tight')\n",
    "        # plt.show()\n",
    "\n",
    "        # 2.2 Distribution Plot:\n",
    "        print('\\n 2.2. Creation of distribution plots..')\n",
    "        fig, axes = plt.subplots(1,3, figsize = (18, 5))\n",
    "\n",
    "        # Employee satisfaction:\n",
    "        axes[0].hist(self.df['satisfaction_level'], bins=30, alpha= 0.7, color= 'skyblue', edgecolor = 'black')\n",
    "        axes[0].set_title('Distribution of employee satisfaction level', fontweight = 'bold')\n",
    "        axes[0].set_xlabel('Satisfaction level')\n",
    "        axes[0].set_ylabel('Frequency')\n",
    "\n",
    "        # Employee Evaluation:\n",
    "        axes[1].hist(self.df['last_evaluation'], bins=30, alpha= 0.7, color= 'lightgreen', edgecolor = 'black')\n",
    "        axes[1].set_title('Distribution of last evaluation', fontweight = 'bold')\n",
    "        axes[1].set_xlabel('Last evaluation score')\n",
    "        axes[1].set_ylabel('Frequency')\n",
    "\n",
    "        # Employee Evaluation:\n",
    "        axes[2].hist(self.df['average_montly_hours'], bins=30, alpha= 0.7, color= 'salmon', edgecolor = 'black')\n",
    "        axes[2].set_title('Distribution of average monthly hours', fontweight = 'bold')\n",
    "        axes[2].set_xlabel('Average monthly hours')\n",
    "        axes[2].set_ylabel('Frequency')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('distribution_plots.png', dpi= 300, bbox_inches = 'tight')\n",
    "        # plt.show()\n",
    "\n",
    "        # 2.3. Bar plot of project count by employee status\n",
    "        print('\\n Creating bar plot for Project count analysis..')\n",
    "        plt.figure(figsize=(10,6))\n",
    "        sns.countplot(data= self.df, x='number_project', hue = 'left', palette=['lightblue', 'salmon'])\n",
    "        plt.title('Employee Project Count Distribution (Stayed vs Left)', fontsize=14, fontweight = 'bold')\n",
    "        plt.xlabel('Number of Projects')\n",
    "        plt.ylabel('Count')\n",
    "        plt.legend(title='Employee Status', labels=['Stayed', 'Left'])\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('project_count_analysis.png', dpi = 300, bbox_inches= 'tight')\n",
    "        # plt.show()\n",
    "\n",
    "        self._print_eda_insights()\n",
    "\n",
    "    def _print_eda_insights(self):\n",
    "        'Print insight from EDA'\n",
    "        print('\\n Insights From EDA: ')\n",
    "        print('-' * 40)\n",
    "\n",
    "        # Project count insights\n",
    "        project_left = self.df[self.df['left']==1]['number_project'].value_counts().sort_index()\n",
    "        project_stayed = self.df[self.df['left']==0]['number_project'].value_counts().sort_index()\n",
    "\n",
    "        print('Peoject count analysis: ')\n",
    "        print(f'Employees who left most commonly worked on {project_left.idxmax()} projects')\n",
    "        print(f'Employees who stayed most commonly worked on {project_stayed.idxmax()} projects')\n",
    "\n",
    "        # Satisfaction insights\n",
    "        avg_satisfaction_left = self.df[self.df['left']== 1]['satisfaction_level'].mean()\n",
    "        avg_satisfaction_stayed = self.df[self.df['left']== 0]['satisfaction_level'].mean()\n",
    "\n",
    "        print(f'\\n Satisfaction Analysis: ')\n",
    "        print(f'Average satisfaction of emplyees who left: {avg_satisfaction_left:.2f}')\n",
    "        print(f'Average satisfaction of emplyees who stayed: {avg_satisfaction_stayed:.2f}')\n",
    "\n",
    "    def employee_clustering(self):\n",
    "        'Plateform k-Mean clustering of employees who left'\n",
    "        print('\\n' + '=' * 60)\n",
    "        print('Employee clustering analysis')\n",
    "        print('\\n' + '=' * 60)\n",
    "\n",
    "        # 3.1 Select relevant cols to filter employees who left\n",
    "        print('\\n 3.1 Selecting employees who left company..')\n",
    "        left_employees = self.df[self.df['left']==1][['satisfaction_level', 'last_evaluation', 'left']]\n",
    "        print(f'Employees who left: {left_employees}')\n",
    "        print(f'Number of Employees who left: {len(left_employees)}')\n",
    "\n",
    "        # 3.2 Perform K-Means clustering\n",
    "        print('\\n 3.2 Perform K-means clustering into 3 clusters..')\n",
    "        clustering_data = left_employees[['satisfaction_level', 'last_evaluation']]\n",
    "\n",
    "        k_means = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "        clusters = k_means.fit_predict(clustering_data)\n",
    "\n",
    "        # Add cluster labels to the data\n",
    "        left_employees_clustered = left_employees.copy()\n",
    "        left_employees_clustered['cluster'] = clusters\n",
    "\n",
    "        # Visualize clusters\n",
    "        plt.figure(figsize=(12,8))\n",
    "        scatter = plt.scatter(left_employees_clustered['satisfaction_level'], left_employees_clustered['last_evaluation'],\n",
    "                              c = clusters, cmap = 'viridis', alpha=0.6, s=50)\n",
    "        \n",
    "        plt.scatter(k_means.cluster_centers_[:,0], k_means.cluster_centers_[:,1], c='red', marker= 'x', s=200, linewidths=3, label='Centorids')\n",
    "\n",
    "        plt.xlabel('Satisfaction level', fontsize=12)\n",
    "        plt.ylabel('Last Evaluation', fontsize=12)\n",
    "        plt.title('K-mean clusterig of Employees who left \\n (Based on Satisfaction & Evaluation)', fontsize=12, fontweight='bold')\n",
    "        plt.colorbar(scatter, label='Cluster')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('employee_clustering.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        # 3.3 Analyze clsuter\n",
    "        self._analyze_clusters(left_employees_clustered, k_means.cluster_centers_)\n",
    "        return left_employees_clustered, k_means\n",
    "\n",
    "    def _analyze_clusters(self, clustered_data, centroids):\n",
    "        '''Analyze and interpret the clusters'''\n",
    "        print('\\n 3.3 Cluster Analysis: ')\n",
    "        print('-' * 40)\n",
    "\n",
    "        for i in range(3):\n",
    "            cluster_data = clustered_data[clustered_data['cluster'] ==i]\n",
    "            avg_satisfaction = cluster_data['satisfaction_level'].mean()\n",
    "            avg_evaluation = cluster_data['last_evaluation'].mean()\n",
    "\n",
    "            count= len(cluster_data)\n",
    "\n",
    "            print(f'\\n Cluster {i}:')\n",
    "            print(f'Count : {count } employees ({count/len(cluster_data) * 100 :.1f}%)')\n",
    "            print(f'Average Satisfaction: {avg_satisfaction:.2f}')\n",
    "            print(f'Average evaluation: {avg_evaluation:.2f}')\n",
    "\n",
    "            # Interpretation\n",
    "\n",
    "            if avg_satisfaction < 0.5 and avg_evaluation < 0.7:\n",
    "                interpretation = 'Low performers with low satisfaction'\n",
    "            elif avg_satisfaction <0.5 and avg_evaluation >=0.7:\n",
    "                interpretation = 'High performers but un-satisfied (burnout risk)'\n",
    "            else: \n",
    "                interpretation = 'Moderately satisfied employees'\n",
    "\n",
    "            print(f'Interpretation: {interpretation}')   \n",
    "\n",
    "    def handle_class_imbalance(self):\n",
    "        '''Handle the class imbalance using SMOTE technique'''\n",
    "\n",
    "        print('\\n' + '=' * 60)\n",
    "        print('4. Handling class Imbalance with SMOTE')\n",
    "        print('=' * 60)\n",
    "\n",
    "        # 4.1 Preprocess data\n",
    "        print('4.1 Preprocessing data')\n",
    "        \n",
    "        # Saperate the categorical and numerical variables\n",
    "        categorical_cols = self.df.select_dtypes(include=['object']).columns.tolist()\n",
    "        numerical_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "        print(f'Categorical columns: {categorical_cols}')\n",
    "        print(f'Numerical columns: {numerical_cols}')\n",
    "\n",
    "        # Remove target variable from the numerical cols:\n",
    "        if 'left' in numerical_cols:\n",
    "            numerical_cols.remove('left')\n",
    "\n",
    "        # Apply get_dummies to categorical variables\n",
    "        df_encoded = pd.get_dummies(self.df[categorical_cols], drop_first=True)\n",
    "\n",
    "        # combined with numerical valriables\n",
    "        X = pd.concat([self.df[numerical_cols], df_encoded], axis=1)\n",
    "        y = self.df['left']\n",
    "\n",
    "        print(f'Final feature matrix shape: {X.shape}')\n",
    "        print(f'Target distribution: \\n{y.value_counts()}')\n",
    "\n",
    "        # 4.2 Startified train-test split\n",
    "\n",
    "        print('\\n 4.2 Performing stratified train-test split (80-20)...')\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size = 0.2, random_state= 123, stratify= y)\n",
    "        \n",
    "        print(f'Train set shape: {self.X_train.shape}')\n",
    "        print(f'Test set shape: {self.X_test.shape}')\n",
    "        print(f'Train target distribution: \\n {self.y_train.value_counts()}')\n",
    "\n",
    "        # 4.3\n",
    "        print('\\n 4.3 Applying SMOTE to balance the training set...')\n",
    "        smote = SMOTE(random_state=123)\n",
    "        self.X_train_balanced, self.y_train_balanced = smote.fit_resample(self.X_train, self.y_train)\n",
    "\n",
    "        print(f'After SMOTE - Train set shape: {self.X_train_balanced.shape}')\n",
    "        print(f'After SMOTE - Target distribution: \\n {pd.Series(self.y_train_balanced).value_counts()}')\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    def train_and_evaluate_models(self):\n",
    "        '''Train models with 5-fold corss-validation'''\n",
    "        print('\\n' + '=' *60)\n",
    "        print('5. Model training and evaluation')\n",
    "        print('=' * 60)\n",
    "\n",
    "        # Initialize models\n",
    "\n",
    "        models = {'Logistic Regression': LogisticRegression(random_state=123, max_iter=1000),\n",
    "                  'Random Forest': RandomForestClassifier(random_state=123, n_estimators=100),\n",
    "                   'Gradient Boosting': GradientBoostingClassifier(random_state=123, n_estimators=100) }\n",
    "        \n",
    "        # 5-fold cross-validation\n",
    "\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "        for name, model in models.items():\n",
    "            print(f'\\n 5.{list(models.keys()).index(name) + 1} Training {name}...')\n",
    "\n",
    "            # Fit model on balanced training data\n",
    "            model.fit(self.X_train_balanced, self.y_train_balanced)\n",
    "            self.models[name] = model\n",
    "\n",
    "            # cross validation scores\n",
    "            cv_score = cross_val_score(model, self.X_train_balanced, self.y_train_balanced, cv = cv, scoring= 'accuracy')\n",
    "\n",
    "            print(f'5-fold CV Accuracy: {cv_score.mean():.4f} (+/- {cv_score.std() * 2:.4f})')\n",
    "\n",
    "            # Prediction on test set\n",
    "            y_pred = model.predict(self.X_test)\n",
    "            # Classification report\n",
    "            print(f'\\n Classification report for {name}: ')\n",
    "            print(classification_report(self.y_test, y_pred))\n",
    "\n",
    "            # Store model performance\n",
    "            self.model_scores[name] = {\n",
    "                'cv_mean': cv_score.mean(),\n",
    "                'cv_std': cv_score.std(),\n",
    "                'predictions': y_pred\n",
    "            }\n",
    "\n",
    "    def identify_best_model(self):\n",
    "        '''Identify best model using ROC/AUC and confuction matrix.'''\n",
    "        print('\\n' + '=' *60)\n",
    "        print('6. Best Model Identification')\n",
    "        print('=' * 60)\n",
    "\n",
    "        # 6.1 ROC/AUC analysis\n",
    "        print('\\n 6.1 ROC/AUC Analysis...')\n",
    "        plt.figure(figsize=(12,8))\n",
    "\n",
    "        auc_scores = {}\n",
    "        for name, model in self.models.items():\n",
    "            # Get prediction probabilities\n",
    "            y_pred_proba = model.predict_proba(self.X_test)[:,1]\n",
    "\n",
    "            # Calculate ROC curve\n",
    "            fpr, tpr, _ = roc_curve(self.y_test, y_pred_proba)\n",
    "            roc_auc  = auc(fpr, tpr)\n",
    "            auc_scores[name] =roc_auc\n",
    "            \n",
    "            # Plot ORC curve\n",
    "            plt.plot(fpr,tpr, linewidth = 2, label= f'{name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "        # plot diagonal line\n",
    "        plt.plot([0,1], [0,1], 'k--', linewidth = 1)\n",
    "        plt.xlim([0.0,1.0])\n",
    "        plt.ylim([0.0,1.05])\n",
    "        plt.xlabel('False Positive Rate', fontsize = 12)\n",
    "        plt.ylabel('True Positive Rate', fontsize = 12)\n",
    "        plt.title('ROC Curves - Model Comparision', fontsize = 14, fontweight = 'bold')\n",
    "        plt.legend(loc= 'lower right')\n",
    "        plt.grid(True, alpha = 0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('roc_curves.png', dpi= 300, bbox_inches = 'tight')\n",
    "        plt.show()\n",
    "\n",
    "        # 6.2 Confusion Matrix\n",
    "        print('\\n 6.2 Confusion Matrix Analysis...')\n",
    "        fig, axes = plt.subplots(1,3,figsize= (18,5))\n",
    "        \n",
    "        from sklearn.metrics import confusion_matrix\n",
    "\n",
    "        for i, (name, model) in enumerate(self.models.items()):\n",
    "            y_pred = self.model_scores[name]['predictions']\n",
    "            cm = confusion_matrix(self.y_test, y_pred)\n",
    "\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap= 'Blues', ax= axes[i])\n",
    "            axes[i].set_title(f'{name} \\n Confusion Matrix', fontweight = 'bold')\n",
    "            axes[i].set_xlabel('Predicted')\n",
    "            axes[i].set_ylabel('Actual')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confusion_matrices.png', dpi= 300, bbox_inches= 'tight')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "        # 6.3 Best model section and metric justification\n",
    "        best_model_name= max(auc_scores, key= auc_scores.get)\n",
    "        best_model = self.models[best_model_name]\n",
    "\n",
    "        print(f'\\n 6.3 Best Model Section')\n",
    "        print('-' * 40)\n",
    "        print(f'Best Model: {best_model_name}')\n",
    "        print(f'AUC score: {auc_scores[best_model_name]:.4f}')\n",
    "\n",
    "        print(f'\\n AUC scores for all models:')\n",
    "        for name, score in sorted(auc_scores.items(), key= lambda x: x[1], reverse= True):\n",
    "            print(f'{name}: {score:.4f}')\n",
    "\n",
    "        self._justify_metrics()\n",
    "\n",
    "        return best_model_name, best_model, auc_scores\n",
    "    \n",
    "    def _justify_metrics(self):\n",
    "        '''Justify the choice of evaluation metrics'''\n",
    "\n",
    "        print(f'\\n Metrics Justification: ')\n",
    "        print('-' * 40)\n",
    "        print('For employee turnover prediction, we should prioritize ReCall Over Precision because:')\n",
    "        print(\"- It's more costly to lose a valuable employee than to invest in retention\")\n",
    "        print(\"- False Negative (missing employees who will leave) are more expensive than False Positives\")\n",
    "        print(\"- We want to identify as many at-risk emploees as possible for proactive retention\")\n",
    "        print(\"- AUC-ROC is ideal as it considers both True Positive Rate and Flase Positive Rate\")\n",
    "\n",
    "\n",
    "    def retention_strategies(self, best_model_name, best_model):\n",
    "        '''Suggest retention strategies based on risk zones'''\n",
    "        print('\\n' + '=' * 60)\n",
    "        print('7. Retention Strategies')\n",
    "        print('=' * 60)\n",
    "\n",
    "        # 7. 1 Predict Probabilities on test data\n",
    "        print('\\n 7.1 Predicting turnover probabilites...')\n",
    "        y_pred_proba = best_model.predict_proba(self.X_test)[:,1]\n",
    "\n",
    "        # Create results dataframe\n",
    "        results_df= pd.DataFrame({\n",
    "            'actual': self.y_test.values,\n",
    "            'probability': y_pred_proba\n",
    "        })\n",
    "\n",
    "        # 7.2 Categorize emplyees into risk zones\n",
    "        print('\\n 7.2 Categorizing employees into risk zones...')\n",
    "        def categories_risk(prob):\n",
    "            if prob < 0.20:\n",
    "                return 'Safe zone (Green)'\n",
    "            elif prob < 0.60:\n",
    "                return 'Low-Risk zone (Yellow)'\n",
    "            elif prob < 0.90:\n",
    "                return 'Medium-Risk zone (Orange)'\n",
    "            else:\n",
    "                return 'High-Risk zone (Red)'\n",
    "\n",
    "        results_df['risk_zone'] = results_df['probability'].apply(categories_risk)\n",
    "\n",
    "\n",
    "        # Analyze risk zones\n",
    "        zone_analysis = results_df['risk_zone'].value_counts()\n",
    "        print('Employee Distribution by Risk Zone: ')\n",
    "        for zone, count in zone_analysis.items():\n",
    "            percentage = (count/ len(results_df)) * 100\n",
    "            print(f'- {zone}: {count} employees ({percentage:.1f} %)')\n",
    "\n",
    "        # Visualize risk zones\n",
    "        plt.figure(figsize=(12,8))\n",
    "        colors= ['green', 'yellow', 'orange', 'red']\n",
    "        zone_order = ['Safe Zone (Green)', 'Low-Risk Zone (Yellow)', 'Medium-Risk Zone (Orange)', 'High-Risk Zone(Red)']\n",
    "\n",
    "\n",
    "        # Filter zones that exist in data\n",
    "        existing_zones = [zone for zone in zone_order if zone in zone_analysis.index]\n",
    "        zone_counts = [zone_analysis[zone] for zone in existing_zones]\n",
    "        zone_colors = colors[:len(existing_zones)]\n",
    "\n",
    "        plt.pie(zone_counts, labels=existing_zones, colors = zone_colors, autopct= '%1.1f%', startangle=90)\n",
    "        plt.title('Employee Distribution by Turnover Risk Zone', fontsize= 14, fontweight = 'bold')\n",
    "        plt.axis('equal')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('risk_zones.png', dpi= 300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        # Retention strategies\n",
    "        self._suggest_retention_strategies()\n",
    "\n",
    "        return results_df\n",
    "    \n",
    "    def _suggest_retention_strategies(self):\n",
    "        '''Provide detailed retention strategies got each risk zone'''\n",
    "        print(f'\\n Retention strategies by Risk Zone: ')\n",
    "        print('=' * 50)\n",
    "\n",
    "        strategies = {\n",
    "            'Safe Zone (Green) - Score < 20%':[\n",
    "                'Maintain current engagement levels',\n",
    "                'Regular check-ins and feedback sessions',\n",
    "                'Provide growth opportunities and skill development',\n",
    "                'Recognition and appreciation program'\n",
    "            ],\n",
    "            'Low-Risk Zone (Yellow) - 20% < Score < 60%':[\n",
    "                'Monitor satisfaction level closely',\n",
    "                'Conduct stay interview to understand concerns',\n",
    "                'Provide additional training and development opportunities',\n",
    "                'Consider workload adjustments if needed'\n",
    "            ],\n",
    "            'Medium-Risk Zone  (Orange) - 60 % < Score < 90 %':[\n",
    "                'Immediate manager intervention required', \n",
    "                'Career development planning and mentoring',\n",
    "                'Flexible work arrengements consideration',\n",
    "                'Compensation and benefits review'\n",
    "            ],\n",
    "            'High-Risk Zone (Red)- Score > 90%':[\n",
    "                'Urgent: Executive / HR immediate attention',\n",
    "                'Comprehensive retention package',\n",
    "                'Role redesign or department transfer options',\n",
    "                'Exit interview preparation if retention fails'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        for zone, actions in strategies.items():\n",
    "            print(f'\\n {zone}:')\n",
    "            for action in actions:\n",
    "                print(f'{action}')\n",
    "\n",
    "\n",
    "    def run_complete_analysis(self):\n",
    "        '''Complete program startup code.'''\n",
    "        print('='* 60 )\n",
    "        print('Starting HR Analysis code')\n",
    "        print('='* 60 )\n",
    "\n",
    "        # Loading the data from CSV file\n",
    "        self.load_data()\n",
    "\n",
    "        # Step 1: Perform Quality check:\n",
    "        self.data_quality_check()\n",
    "\n",
    "\n",
    "        # Step 2: EDA\n",
    "        self.exploratory_data_analytics()\n",
    "\n",
    "        # Step 3: Clustering\n",
    "        self.employee_clustering()\n",
    "\n",
    "        # Step 4: Handle class imbalance\n",
    "        self.handle_class_imbalance()\n",
    "\n",
    "        # Step 5: Model training\n",
    "        self.train_and_evaluate_models()\n",
    "\n",
    "        # Step 6: Best Model identification:\n",
    "        best_model_name, best_model, auc_score = self.identify_best_model()\n",
    "\n",
    "        # Step 7: Retention strategies\n",
    "        result_df = self.retention_strategies(best_model_name, best_model)\n",
    "\n",
    "        print('\\n' + '=' *60)\n",
    "        print('Analysis Completed')\n",
    "        print('=' * 60)\n",
    "        print('Generated Visualiztions: ')\n",
    "        print('correlation_heatmap.png')\n",
    "        print('distribution_plots.png')\n",
    "        print('project_count_analysis.png')\n",
    "        print('employee_clustering.png')\n",
    "        print('roc_curves.png')\n",
    "        print('confusion_matrics.png')\n",
    "        print('risk_zones.png')\n",
    "\n",
    "        return result_df, best_model_name, best_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    analyzer = HRAnalyzer('HR_Comma_sep.csv')\n",
    "    result, best_model_name, best_model = analyzer.run_complete_analysis()\n",
    "\n",
    "    print(f'\\n Final Results Summary:')\n",
    "    print(f'Best Model: {best_model_name}')\n",
    "    print(f'Analysis completed successfully!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
