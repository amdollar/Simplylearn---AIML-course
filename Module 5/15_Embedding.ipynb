{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b9c9419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. loading the text file (document from the files)\n",
    "# 2. Perform Splitting in chunks\n",
    "# 3. Peform Embedding on these chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12636e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28234b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'demo.txt'}, page_content='This is an example of using text file in langchain\\nlangchain is a powerful framework')]\n"
     ]
    }
   ],
   "source": [
    "# 1. Loaded file data \n",
    "loader = TextLoader('demo.txt')\n",
    "doc = loader.load()\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "248e44b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chunk 1: \n",
      " This \n",
      " --------------------\n",
      " Chunk 2: \n",
      " is \n",
      " --------------------\n",
      " Chunk 3: \n",
      " an \n",
      " --------------------\n",
      " Chunk 4: \n",
      " exam \n",
      " --------------------\n",
      " Chunk 5: \n",
      " xampl \n",
      " --------------------\n",
      " Chunk 6: \n",
      " mple \n",
      " --------------------\n",
      " Chunk 7: \n",
      " of \n",
      " --------------------\n",
      " Chunk 8: \n",
      " usin \n",
      " --------------------\n",
      " Chunk 9: \n",
      " sing \n",
      " --------------------\n",
      " Chunk 10: \n",
      " text \n",
      " --------------------\n",
      " Chunk 11: \n",
      " file \n",
      " --------------------\n",
      " Chunk 12: \n",
      " in \n",
      " --------------------\n",
      " Chunk 13: \n",
      " lang \n",
      " --------------------\n",
      " Chunk 14: \n",
      " angch \n",
      " --------------------\n",
      " Chunk 15: \n",
      " gchai \n",
      " --------------------\n",
      " Chunk 16: \n",
      " hain \n",
      " --------------------\n",
      " Chunk 17: \n",
      " lang \n",
      " --------------------\n",
      " Chunk 18: \n",
      " angch \n",
      " --------------------\n",
      " Chunk 19: \n",
      " gchai \n",
      " --------------------\n",
      " Chunk 20: \n",
      " hain \n",
      " --------------------\n",
      " Chunk 21: \n",
      " is a \n",
      " --------------------\n",
      " Chunk 22: \n",
      " powe \n",
      " --------------------\n",
      " Chunk 23: \n",
      " owerf \n",
      " --------------------\n",
      " Chunk 24: \n",
      " erful \n",
      " --------------------\n",
      " Chunk 25: \n",
      " fram \n",
      " --------------------\n",
      " Chunk 26: \n",
      " ramew \n",
      " --------------------\n",
      " Chunk 27: \n",
      " mewor \n",
      " --------------------\n",
      " Chunk 28: \n",
      " work \n",
      " --------------------\n"
     ]
    }
   ],
   "source": [
    "# 2. Splitter:\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 5, chunk_overlap= 3)\n",
    "chunks = splitter.split_documents(doc)\n",
    "\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f' Chunk {i+1}: \\n {chunk.page_content} \\n {'-' * 20}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c3b107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk list prepared as per the splitting : \n",
      " ['This', 'is', 'an', 'exam', 'xampl', 'mple', 'of', 'usin', 'sing', 'text', 'file', 'in', 'lang', 'angch', 'gchai', 'hain', 'lang', 'angch', 'gchai', 'hain', 'is a', 'powe', 'owerf', 'erful', 'fram', 'ramew', 'mewor', 'work']\n",
      "Open AI Vector shape: 28 chunks x 1536 dims\n"
     ]
    }
   ],
   "source": [
    "# 3. Embedding\n",
    "# Import Embedding package, \n",
    "# Have list of all the chunks\n",
    "# embed this list of chunks \n",
    "# print for illustration \n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "chunk_list = [c.page_content for c in chunks]\n",
    "print(f'Chunk list prepared as per the splitting : \\n {chunk_list}')\n",
    "\n",
    "vectors = embedding.embed_documents(chunk_list)\n",
    "# print(f' Embedded integers of the chunk list: \\n {vectors}')\n",
    "\n",
    "print(f'Open AI Vector shape: {len(vectors) } chunks x {len(vectors[0])} dims')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21b4b4c",
   "metadata": {},
   "source": [
    "One More step forward, we will see how to compute similarity (Cosine) b/w query and vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a23f83ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embedding : [0.0006628383416682482, 0.025741448625922203, 0.007136902771890163, 0.03336041420698166, -0.03193381801247597, -0.027387520298361778, -0.005036199931055307, -0.040007416158914566, 0.006889991462230682, -0.03235709294676781]\n",
      "OpenAI Cosine similarities: [0.22935791 0.17299077 0.20265067 0.15566929 0.15858994 0.20354475\n",
      " 0.13930213 0.16244068 0.22398218 0.17405944 0.17203865 0.16569891\n",
      " 0.17863051 0.24680803 0.23192548 0.17428342 0.17863051 0.24680803\n",
      " 0.23192548 0.17428342 0.21975848 0.12811939 0.11822654 0.19905233\n",
      " 0.17930245 0.30871256 0.13441275 0.17606955]\n",
      "Most similar part: ramew\n"
     ]
    }
   ],
   "source": [
    "# 1. Get the user query\n",
    "# 2. Convert it into vector\n",
    "# 3. Calculate the cosine b/w query vector and the already existing document vector. \n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "query = \"What is RAG?\"\n",
    "\n",
    "q_vector= embedding.embed_query(query)\n",
    "\n",
    "print(f'Query embedding : {q_vector[:10]}')\n",
    "\n",
    "\n",
    "sims = cosine_similarity([q_vector], vectors)[0]\n",
    "print(\"OpenAI Cosine similarities:\", sims)\n",
    "\n",
    "top_index = int(np.argmax(sims))\n",
    "print(f'Most similar part: {chunks[top_index].page_content}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
