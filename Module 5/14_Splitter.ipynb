{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ea79c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes due to LLM Context window size also, to get Best out of a LLM Model, it is good to split the text that is provided to the LLM.\n",
    "\n",
    "# It is better to split the text into small pieces referred as \"Chunks\"\n",
    "\n",
    "# Splitting makes sure that your reterival step pulls relevant, concise pieces rather than entire document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23e95fc",
   "metadata": {},
   "source": [
    "Goal: Convert a large document into small managable chunks that are relevant for 'Embedding' and 'Reterieval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe2fbf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da6a9b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'demo.txt'}, page_content='This is an example of using text file in langchain\\nlangchain is a powerful framework')]\n",
      " Chunk 1: \n",
      " This \n",
      " --------------------\n",
      " Chunk 2: \n",
      " is \n",
      " --------------------\n",
      " Chunk 3: \n",
      " an \n",
      " --------------------\n",
      " Chunk 4: \n",
      " exam \n",
      " --------------------\n",
      " Chunk 5: \n",
      " xampl \n",
      " --------------------\n",
      " Chunk 6: \n",
      " mple \n",
      " --------------------\n",
      " Chunk 7: \n",
      " of \n",
      " --------------------\n",
      " Chunk 8: \n",
      " usin \n",
      " --------------------\n",
      " Chunk 9: \n",
      " sing \n",
      " --------------------\n",
      " Chunk 10: \n",
      " text \n",
      " --------------------\n",
      " Chunk 11: \n",
      " file \n",
      " --------------------\n",
      " Chunk 12: \n",
      " in \n",
      " --------------------\n",
      " Chunk 13: \n",
      " lang \n",
      " --------------------\n",
      " Chunk 14: \n",
      " angch \n",
      " --------------------\n",
      " Chunk 15: \n",
      " gchai \n",
      " --------------------\n",
      " Chunk 16: \n",
      " hain \n",
      " --------------------\n",
      " Chunk 17: \n",
      " lang \n",
      " --------------------\n",
      " Chunk 18: \n",
      " angch \n",
      " --------------------\n",
      " Chunk 19: \n",
      " gchai \n",
      " --------------------\n",
      " Chunk 20: \n",
      " hain \n",
      " --------------------\n",
      " Chunk 21: \n",
      " is a \n",
      " --------------------\n",
      " Chunk 22: \n",
      " powe \n",
      " --------------------\n",
      " Chunk 23: \n",
      " owerf \n",
      " --------------------\n",
      " Chunk 24: \n",
      " erful \n",
      " --------------------\n",
      " Chunk 25: \n",
      " fram \n",
      " --------------------\n",
      " Chunk 26: \n",
      " ramew \n",
      " --------------------\n",
      " Chunk 27: \n",
      " mewor \n",
      " --------------------\n",
      " Chunk 28: \n",
      " work \n",
      " --------------------\n"
     ]
    }
   ],
   "source": [
    "# 1. Load a text document from the local\n",
    "\n",
    "\n",
    "\n",
    "loader = TextLoader('demo.txt')\n",
    "docs = loader.load()\n",
    "print(docs)\n",
    "\n",
    "\n",
    "# 2. Create chunks from the data \n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 5, chunk_overlap=3)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f' Chunk {i+1}: \\n {chunk.page_content} \\n {'-' * 20}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cdb07d",
   "metadata": {},
   "source": [
    "Onces the splitting of the doc is completed, next step is to embed the text into numbers, that will be filled to the \n",
    "Vector databse, and then will be used in an RAG based application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90668435",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
